{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image search with autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Autoencoders are an architecture that have a variety of applications from [denoising data](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf) to [generative models](https://arxiv.org/1312.6114.pdf).\n",
    "\n",
    "In this post we will look at how to search for images using autoencoders. The two papers that guide this post are [Semantic Hashing](http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf) and [Using Very Deep Autoencoders for Content-Based Image Retrieval](http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders, the gist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoders considered in this post are simply neural networks that learn the mapping $f:X\\mapsto X$. Sounds too easy doesn't it? And why would we want to learn to predict the data we already have?\n",
    "\n",
    "The key idea behind the architecture of an autoencoder is that a \"bottleneck\" is placed in the middle of the network. This implicitly forces the network to learn a compressed representation of the input data, which is what we are actually interested in.\n",
    "\n",
    "More specifically we actually think of the autoencoder as two pieces, the _encoder_ which is the first half of the network mapping $X\\in \\mathbb{R}^{n}$ to some vector in $\\mathbb{R}^{d}$ and the _decoder_ which maps that vector back to $\\mathbb{R}^{n}$. The vector output by the encoder is called the _latent representation_ of the data which is what we will use in our image search. Again, at the end of the day all we really care about is the output of the bottleneck.\n",
    "\n",
    "This picture below should clear up any ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./static/autoencoder-architecture.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One really nice thing about autoencoders is that the idea described above is very intuitive. Suppose our data $X$ lives in $\\mathbb{R}^{n}$ and one of the hidden layers of $f$ outputs a vector of dimmension $d<n$. Then, intuitively it makes sense that if $f$ reconstructs $X$ well after reducing the input in dimmensionality then the output of that hidden layer must contain a lot of information about $X$ in order to reproduce $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders for searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we apply autoencoders to searching?\n",
    "\n",
    "Let's consider the use case where we have a set of images $X$, and for a given image $q$ we would like to retrieve all images in $X$ that are \"similar\" to $q$.\n",
    "\n",
    "One such way to do this with an autoencoder is to keep a list of key-value pairs of each image $x\\in X$ and its encoded representation $\\hat{x}$. Recall these codes are simply vectors, so we can compute similarity scores of $\\hat{q}$ and each $\\hat{x}$, returning the images which scored highest. This is already more efficient than simply computing the similarity scores on the raw values in $X$ in the sense that the encoded representations are much smaller in dimmension. However, both methods are linear searches with time complexity $O(|X|)$, that is the more images we have in our database, the longer the search will take and the less benefit we see in performance from computing the similarity scores on smaller vectors.\n",
    "\n",
    "It turns out we can still use the codes to perform a faster search in constant time, we just need to add one extra step. We now define the encoder as $\\phi:\\mathbb{R}^{n}\\mapsto\\{0,1\\}^{d}$ ($d$-bit binary codes). Once again we keep a list of key-value pairs, but this time the key is a $d$-bit integer rather than a real valued vector of dimmension $d$. Then for our query image $q$ we now return all images $x\\in X$ such that the hamming distince (number of bits that differ) of $\\phi(q)$ and $\\phi(x)$ is less than some predetermined threshold $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few remarks on building and training the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning binary codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be asking how does one constrain the neural network to learn binary codes. It turns out this is actually rather simple. Several techniques, such as adding gaussian noise to the mini-batches, for doing this are discussed in [Semantic Hashing](http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf) however the simplest method, described in [here](http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf), is to feed the outputs of the hidden layer which outputs the latent vector into a softmax, round those values up to 1 or down to 0 and then compute the gradient update as if the rounding never occurred.\n",
    "\n",
    "In the keras we can create a custom layer to this for as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(x):\n",
    "    return x + K.stop_gradient(K.round(x) - x)\n",
    "Binarize = Lambda(binarize, output_shape=lambda x: x, name='encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the use case it may be perfectly acceptable to train on the same data we intend to search over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the latent dimmension and threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the MNIST digits data set as an example of working out a heuristic for choosing the latent dimmension and threshold since that's what we'll be working with below. In the train split provided by keras there are $60,000$ images. If the number of possible binary codes output by the autoencoder is $2^d=N$, then we will have approximately $60,000/N$ images stored per code (assuming the images are distributed uniformly across the codes). In the papers referenced in the beginning of this post the author's found good results when there were about 0.4 items per code so we'll choose $d=17$ which gives us a total of $N=131072$ codes and approximately $0.46$ images per code.\n",
    "\n",
    "Now, lets choose our threshold as $t=3$, then we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "from utils import *\n",
    "X_train, X_test = fetch_data()\n",
    "show_one(choices(X_train, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from autoencoding import AutoEncoder\n",
    "INPUT_DIM = X_train.shape[-1]\n",
    "LATENT_DIM = 17\n",
    "INTERMEDIATE_DIMS = [1000, 700, 300,  150, 75]\n",
    "OUTPUT_ACTIVATION = 'sigmoid'\n",
    "EPOCHS = 125\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(\n",
    "    INPUT_DIM,\n",
    "    LATENT_DIM,\n",
    "    INTERMEDIATE_DIMS,\n",
    "    OUTPUT_ACTIVATION)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='binary_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=250,\n",
    "                batch_size=256,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = choices(X_test, k=10)\n",
    "reconstructions = autoencoder.predict(test_sample)\n",
    "show_side_by_side(test_sample, reconstructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = autoencoder.encoder\n",
    "import keras\n",
    "encoder = keras.models.Model(\n",
    "            inputs=autoencoder.input,\n",
    "            outputs=autoencoder.get_layer('encoder').get_output_at(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear search over test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_sim(v1, v2):\n",
    "    return v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define key-value pairs of encoding, image for search\n",
    "kvps = [(code, image) for code, image in zip(encoder.predict(X_test), X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "# pick on image from test set\n",
    "query_img = random.choice(X_test).reshape(1, 784)\n",
    "\n",
    "# retrieve similar images\n",
    "query_code = encoder.predict(query_img)\n",
    "sims = [(cosine_sim(query_code, stored_code), image) for stored_code, image in kvps]\n",
    "# get top 10 images\n",
    "sorted_sims = sorted(sims, reverse=True, key=lambda x: x[0])\n",
    "top_matches = [img for _, img in sorted_sims[:10]]\n",
    "\n",
    "show_side_by_side(query_img, top_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the codes are binary aren't they?\n",
    "query_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numba\n",
    "\n",
    "# see,\n",
    "# https://en.wikipedia.org/wiki/Hamming_distance#Algorithm_example\n",
    "@numba.jit\n",
    "def hamming_distance(n1, n2):\n",
    "    # this number is made of each bit in either n1 or n2\n",
    "    # but not both\n",
    "    v = n1 ^ n2\n",
    "    d = 0\n",
    "    while v != 0:\n",
    "        # subtracting 1 clears the least bit, a, in v and sets all bits\n",
    "        # before a which are cleared by the logical &\n",
    "        # 2^n = sum(2^m for 0 <= m <= n-1)\n",
    "        d += 1\n",
    "        v &= v - 1\n",
    "    return d\n",
    "\n",
    "\n",
    "class DB:\n",
    "    def __init__(self, encoder, items=None):\n",
    "        self.encoder = encoder\n",
    "        output_dim = self.encoder.output_shape[-1]\n",
    "        self._z = np.array([2**i for i in range(output_dim)], dtype=np.uint32).transpose()\n",
    "        self._db = collections.defaultdict(list)\n",
    "        self.store_items(items)\n",
    "\n",
    "    def search(self, item, threshold=3, top_n=10):\n",
    "        key = self._make_keys(item)[0]\n",
    "        hits = self._find_hits(key, threshold)\n",
    "        items = self._fetch_items(hits, top_n)\n",
    "        return items\n",
    "\n",
    "    @numba.jit\n",
    "    def _find_hits(self, key, threshold):\n",
    "        hits = collections.defaultdict(int)\n",
    "        for other_key in self._db:\n",
    "            dist = hamming_distance(other_key, key)\n",
    "            if dist <= threshold:\n",
    "                hits[other_key] += 2**(threshold-dist)\n",
    "        return hits\n",
    "\n",
    "    def _fetch_items(self, hits, top_n):\n",
    "        out = []\n",
    "        sorted_hits = sorted(hits.items(), key=lambda x: x[1])\n",
    "        while len(out) < top_n:\n",
    "            for key in sorted_hits:\n",
    "                for item in self._db[key]:\n",
    "                    out.append(item)\n",
    "        return out\n",
    "\n",
    "    def store_items(self, items):\n",
    "        keys = self._make_keys(items)\n",
    "        for key, item in zip(keys, items):\n",
    "            self._db[key].append(item)\n",
    "\n",
    "    def _make_keys(self, items):\n",
    "        codes = self.encoder.predict(items)\n",
    "        return np.dot(codes, self._z).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "db = DB(encoder, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_img = random.choice(X_train).reshape(1, -1)\n",
    "res = db.search(query_img, threshold=3)\n",
    "show_side_by_side(query_img, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
