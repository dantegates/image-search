{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image search with autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Autoencoders are an architecture that have a variety of applications from [denoising data](http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf) to [generative models](http://kvfrans.com/variational-autoencoders-explained/).\n",
    "\n",
    "In this post we will look at how to search for similar images given a query image with autoencoders and implement one with [keras](http://keras.io). While we will focus on image searches autoencoders have been shown to work well in other settings as well, for example text-based problems such as retrieving for relevant news articles (see the resource section at the end of this post)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are simply neural networks that learn the mapping $f:X\\mapsto X$. Does that sound too easy? Are you wondering why we would want to predict the input we are already given?\n",
    "\n",
    "The key idea that makes this mapping useful is that a \"bottleneck\" is placed in the middle of the network which implicitly forces the network to learn a compressed representation of the input data. It is important to note that this is the only output from the network that we are actually interested in.\n",
    "\n",
    "More specifically we think of an autoencoder as two pieces. First there is the _encoder_, the first half of the network that maps $X\\in \\mathbb{R}^{n}$ to some vector in $\\mathbb{R}^{d}$ where $d<n$. Second we have the _decoder_ which maps that vector back to the orignal feature space $\\mathbb{R}^{n}$. The vector output by the encoder is known as the _latent representation_ of the data and is what we will use to search for images by. Again, it is important to understand that at the end of the day all we really care about is the output of the bottleneck.\n",
    "\n",
    "In keras implementing an autoencoder is (almost) as simple as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import keras\n",
    "model = keras.models.Sequential([\n",
    "    Dense(1000),\n",
    "    Dense(500),\n",
    "    Dense(250),\n",
    "    Dense(64),\n",
    "    Dense(250),\n",
    "    Dense(500),\n",
    "    Dense(1000)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of the middle layer `Dense(64)` is the latent representation, or code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice thing about autoencoders is that the idea described above is very intuitive. Suppose our data $X$ lives in $\\mathbb{R}^{n}$ and one of the hidden layers of $f$ outputs a vector of dimmension $d<n$. Then, it follows that if $f$ reconstructs $X$ well after reducing the input in dimmensionality then that $d$-dimmensional vector must contain a lot of information about the input itself in order to reproduce it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders for searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do autoencoders apply to searching?\n",
    "\n",
    "Let's consider the use case where we have a set of images $X$, and for a given image, $q$, we would like to retrieve all images in $X$ that are \"similar\" to $q$.\n",
    "\n",
    "One such way to do this with an autoencoder is to keep a list of key-value pairs of each image $x\\in X$ and its encoded representation $\\hat{x}$ (the output of the latent layer when we predict on $x$). Recall these codes are simply vectors, so we can compute similarity scores of $\\hat{q}$ with each $\\hat{x}$, returning the images which scored highest.\n",
    "\n",
    "Now, this method is already more efficient than simply computing the similarity scores over the raw values in $X$ - in the sense that the encoded representations are much smaller in dimmension. Nevertheless, both approaches run in linear time, that is the more images we have in our database, the longer the search will take and the less benefit we see in performance from computing the similarity scores on smaller vectors.\n",
    "\n",
    "It turns out we can still use the codes to perform a faster search in constant time, we just need to build the autoencoder with one slight modification. Instead of mapping the input to vectors of dimmension $d$, we define the encoder as $\\phi:\\mathbb{R}^{n}\\mapsto\\{0,1\\}^{d}$ ($d$-bit binary codes). To perform the search over these KVPs we implement a data scructure called a [\"semantic hashing table\"](http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf). Don't let the name intimidate you, the data scructure is essentially a hash map where the keys are $d$-bit integers (converted from the output of $\\phi$) and the values are lists (at least that's the simple python implementation) and is initialized with the following algorithm.\n",
    "\n",
    "```python\n",
    "A  = collections.defaultdict(list)\n",
    "for image in images:\n",
    "    code = phi(image)\n",
    "    A[code].append(image)\n",
    "```\n",
    "\n",
    "Now, for our query image $q$, we can return all images $x\\in X$ such that the hamming distince (number of bits that differ) of $\\phi(q)$ and $\\phi(x)$ is less than some predetermined threshold $t$. The following python snippet shows how this is done in constant time.\n",
    "\n",
    "```python\n",
    "results = list()\n",
    "q_hat = phi(q)\n",
    "for key, images in A.items():  # A has no more than 2^d keys\n",
    "    if hamming_distance(q_hat, key) < t:\n",
    "       results.extend(images)\n",
    "```\n",
    "\n",
    "In this section we've oversimplified the implementation just a bit. To get a full idea of how to implement this search see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few remarks on building and training the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning binary codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be asking how does one constrain the neural network to learn binary codes. It turns out this is actually rather simple. Several techniques are discussed in [semantic hashing](http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf), however the simplest method, described [here](http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf), is to round the outputs of the encoder up to 1 or down to 0 (these will already be values between 0 and 1 since we use a softmax for the activation of this layer) and then compute the gradient update as if the rounding never occurred.\n",
    "\n",
    "We can do this in keras with a custom layer using `keras.backend.round()` and `keras.backend.stop_gradient()` as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import keras\n",
    "from keras.layers import Lambda\n",
    "def binarize(x):\n",
    "    return x + K.stop_gradient(K.round(x) - x)\n",
    "Binarize = Lambda(binarize, output_shape=lambda x: x, name='encoding')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the latent dimmension and threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When choosing the size of the latent dimmension it is useful to consider the following heuristic. As a specific example we'll consider searching over the MNIST digits since that's the data we'll work with below. Specifically we will use the MNIST training set provided by `keras.datasets.mnist` which contains 60,000 images.\n",
    "\n",
    "In the papers referenced at the end of this post the author's found good results when each code mapped to approximately 0.4 items (images in our case). Following this pattern, we will chose $d$ such that $60000/2^{d}\\approx 0.4$, that is $d=17$. The key idea is for $d$ to be large enough for the model to train well, yet small enough that we will actually find images with codes within a given hamming distance of our query code.\n",
    "\n",
    "Choosing the threshold $t$ will depend on the use case. Just keep in mind that the number of images, $n$, you should expect to retrieve for a given query is\n",
    "\n",
    "$$n\\approx\\frac{|X|}{2^{d}}\\sum_{i=0}^{t}{\\binom{d}{i}}$$\n",
    "\n",
    "In our case, choosing $t=3$ this is approximately $0.46*834\\approx 384$ images.\n",
    "\n",
    "(If you're paying attention you will have noticed that we assume the model will distribute the images uniformly accross the $d$-bit space. If the autoencoder reconstructs the input images well then this is reasonable to assume since the autoencoder would have to \"spread out\" the images accross the codes since the final output of the autoencoder is determined by the codes. At least, this holdes for our case where we know that the digits are uniformly distributed in the training set.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in some cases there may not be a need for the autoencoder to generalize to new data if the entire database that will be searched over is available at train time. Thus overfitting to the training data would only improve the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll take a look at implementing an autoencoder in keras. I won't share the full source code, which can be found [here](https://github.com/dantegates/image-search), but I will highlight the main points and look at the results.\n",
    "\n",
    "First we'll import the `AutoEncoder` class that I've defined (which is a sublcass of the `keras.models.Model` and look at its class signature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module autoencoders:\n",
      "\n",
      "__init__(self, input_dim, latent_dim, intermediate_dims, output_activation)\n",
      "    Initialize an ``AutoEncoder``.\n",
      "    \n",
      "    Args:\n",
      "        input_dim (int): Dimension of the input.\n",
      "        latent_dim (int): Dimension of the \"latent representation\" or code.\n",
      "        intermediate_dims (list): List of `int`s representing the\n",
      "            dimmension of the hidden layers up to, but not including, the\n",
      "            latent layer. See the example below.\n",
      "        output_activation (str or object): The activation used on the final\n",
      "            output of the autoencoder. This gets passed on to the underlying\n",
      "            `keras` implementation.\n",
      "    \n",
      "    Example\n",
      "    -------\n",
      "    The instance\n",
      "    \n",
      "    >>> autoencoder = AutoEncoder(784, 32, [256, 128])\n",
      "    \n",
      "    will have the following architecture ::\n",
      "        \n",
      "        |--------- 784 ---------|       INPUT\n",
      "    \n",
      "           |------ 256 ------|\n",
      "    \n",
      "              |--- 128 ---|\n",
      "    \n",
      "                |-  32 -|               CODE\n",
      "    \n",
      "              |--- 128 ---|\n",
      "    \n",
      "           |------ 256 ------|\n",
      "    \n",
      "        |--------- 784 ---------|       OUTPUT\n",
      "    \n",
      "    \n",
      "    Usage\n",
      "    -----\n",
      "    >>> autoencoder = AutoEncoder(...)\n",
      "    >>> autoencoder.compile(...)\n",
      "    >>> autoencoder.fit(X_train, X_train, ...)\n",
      "    >>> encoder = autoencoder.integer_encoder(X)\n",
      "    ... np.array([[12387],\n",
      "                  [3982909],\n",
      "                  ...])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from autoencoders import AutoEncoder\n",
    "help(AutoEncoder.__init__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation uses `keras` functional API by looping over the intermediate dimmensions and \"stacking\" `keras.layers.Dense` layers on top of each other. You can read more about the functional API in the [docs](https://keras.io/getting-started/functional-api-guide/). My implementation is similar to the keras [tutorial](https://blog.keras.io/building-autoencoders-in-keras.html) building autoencoders with one improvement. Rather than keeping track of the encoder and decoder as separate objects, I simply name the encoder layer, i.e.\n",
    "\n",
    "```python\n",
    "Binarize = Lambda(binarize, output_shape=lambda x: x, name=ENCODING_LAYER_NAME)\n",
    "```\n",
    "\n",
    "Then we can implement a property that grabs this layer by name and instantiates another `keras.models.Model` when we want to use the encoder after training\n",
    "\n",
    "```python\n",
    "class AutoEncoder(keras.models.Model):\n",
    "    ...\n",
    "    @property\n",
    "    def bit_encoder(self):\n",
    "        encoding = self.get_layer(ENCODING_LAYER_NAME).get_output_at(-1)\n",
    "        return keras.models.Model(inputs=self.input, outputs=encoding)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gone over the implementation, let's train the model and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel_values = X_train.mean(axis=0)\n",
    "std_all_pixel_values = X_train.std()\n",
    "X_train = (X_train - mean_pixel_values) / std_all_pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_367 (Conv2D)          (None, 8, 8, 336)         258384    \n",
      "_________________________________________________________________\n",
      "conv2d_368 (Conv2D)          (None, 8, 8, 1024)        345088    \n",
      "_________________________________________________________________\n",
      "conv2d_369 (Conv2D)          (None, 8, 8, 512)         524800    \n",
      "_________________________________________________________________\n",
      "conv2d_370 (Conv2D)          (None, 8, 8, 256)         131328    \n",
      "_________________________________________________________________\n",
      "conv2d_371 (Conv2D)          (None, 8, 8, 128)         32896     \n",
      "_________________________________________________________________\n",
      "conv2d_372 (Conv2D)          (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv2d_373 (Conv2D)          (None, 8, 8, 28)          1820      \n",
      "_________________________________________________________________\n",
      "conv2d_374 (Conv2D)          (None, 8, 8, 64)          1856      \n",
      "_________________________________________________________________\n",
      "conv2d_375 (Conv2D)          (None, 8, 8, 128)         8320      \n",
      "_________________________________________________________________\n",
      "conv2d_376 (Conv2D)          (None, 8, 8, 256)         33024     \n",
      "_________________________________________________________________\n",
      "conv2d_377 (Conv2D)          (None, 8, 8, 512)         131584    \n",
      "_________________________________________________________________\n",
      "conv2d_378 (Conv2D)          (None, 8, 8, 1024)        525312    \n",
      "_________________________________________________________________\n",
      "conv2d_379 (Conv2D)          (None, 8, 8, 336)         344400    \n",
      "_________________________________________________________________\n",
      "conv2d_380 (Conv2D)          (None, 8, 8, 3)           1011      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 2,348,079\n",
      "Trainable params: 2,348,079\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "autoencoder = keras.models.Sequential([\n",
    "    # use same padding so we can get back to original size of 32x32\n",
    "    # valid padding will give us 9x9\n",
    "    keras.layers.Conv2D(filters=336, kernel_size=16, strides=4, padding='same', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Conv2D(filters=1024, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=128, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=28, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=1, padding='valid'),    \n",
    "    keras.layers.Conv2D(filters=128, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=256, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=512, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=1024, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=336, kernel_size=1, padding='valid'),\n",
    "    keras.layers.Conv2D(filters=3, kernel_size=1, padding='valid'),\n",
    "    keras.layers.UpSampling2D(size=(4, 4))\n",
    "])\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "training_history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=5,\n",
    "    batch_size=256,\n",
    "    verbose=1,  # set verbosity to 0 so this post doesn't get cluttered\n",
    "                # we'll look at the loss from the keras history\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just show the loss every 25 epochs\n",
    "training_history.history['loss'][::25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train results\n",
    "Now that we have trained the autoencoder, lets randomly sample images from the test set and see how well it can reconstruct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly sample 10 images and look at output\n",
    "test_sample = choices(X_train, k=10)\n",
    "reconstructions = autoencoder.predict(test_sample)\n",
    "show_side_by_side(test_sample, reconstructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear search\n",
    "The model looks like it has trained well. Now we can pull out the encoder from the model and take a look at how it performs using a simple linear search using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = autoencoder.bit_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# define key-value pairs of encoding, image for search\n",
    "kvps = [(code, image) for code, image in zip(encoder.predict(X_train), X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "# pick on image from test set\n",
    "q = random.choice(X_train).reshape(1, 784)\n",
    "\n",
    "# retrieve similar images\n",
    "query_code = encoder.predict(QUERY_IMG)\n",
    "sims = [(cosine_sim(query_code, stored_code), image) for stored_code, image in kvps]\n",
    "# get top 10 images\n",
    "sorted_sims = sorted(sims, reverse=True, key=lambda x: x[0])\n",
    "top_matches = [img for _, img in sorted_sims[:10]]\n",
    "\n",
    "show_side_by_side(QUERY_IMG, top_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the codes are binary aren't they?\n",
    "query_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic hashing\n",
    "\n",
    "Lastly, we'll take a look at search results given the same query image, but using the \"semantic hash table.\" (implemented in [semantic_hashing.py](https://github.com/dantegates/image-search/blob/master/semantic_hashing.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_hashing import DB\n",
    "encoder = autoencoder.integer_encoder  # keras model that converts binary code to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "db = DB(encoder, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some values that make use feel good about our choice of $d$\n",
    "db.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "res = db.search(q, threshold=1)\n",
    "show_side_by_side(q, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even on this relatively small data set we already observe much faster lookups without any noticable impact to the search performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "For further information see the papers which introduced autoencoders and guided this post are\n",
    "\n",
    "- [Semantic Hashing](http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf)\n",
    "- [Using Very Deep Autoencoders for Content-Based Image Retrieval](http://www.cs.toronto.edu/~fritz/absps/esann-deep-final.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
